{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"v1.Tanh.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"d60c68c6-5dc7-4db9-b711-7b413973505b"},"source":["### Set Env"],"id":"d60c68c6-5dc7-4db9-b711-7b413973505b"},{"cell_type":"code","metadata":{"id":"92664c0f-f950-4b8f-8b95-e7eab98dea6d"},"source":["! pip install ipywidgets\n","! pip install transformers\n","! pip install wandb\n","! pip install adamp\n","! mkdir custom_data"],"id":"92664c0f-f950-4b8f-8b95-e7eab98dea6d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fcf3a9a-27fb-40b2-9326-3451d38bdefd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619025596740,"user_tz":-540,"elapsed":14999,"user":{"displayName":"이현규","photoUrl":"","userId":"11926500847670507435"}},"outputId":"05a48d82-1f6d-4ee2-96ea-36ec3a737e30"},"source":["# Import libraries\n","import pandas as pd\n","import numpy as np\n","import os\n","import time\n","import math\n","import glob\n","import pickle\n","import random\n","import argparse\n","from pathlib import Path\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from torch.autograd import Variable\n","from torch.optim.lr_scheduler import _LRScheduler\n","from adamp import AdamP\n","import transformers\n","import wandb\n","import warnings\n","\n","# Ignore Warnings\n","warnings.filterwarnings(action='ignore')\n","\n","# Set device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f'device : {device}')\n","print(torch.cuda.get_device_properties(device))\n","\n","# Set ROOT_PATH\n","ROOT_PATH = os.getcwd()\n","print(f'ROOT_PATH : {ROOT_PATH}')\n","    \n","# Set wandb\n","wandb.login()\n","CFG = wandb.config\n","%env WANDB_PROJECT = P2\n","%env WANDB_LOG_MODEL = true\n","%env WANDB_SILENT = true"],"id":"5fcf3a9a-27fb-40b2-9326-3451d38bdefd","execution_count":null,"outputs":[{"output_type":"stream","text":["device : cuda:0\n","_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n","ROOT_PATH : /content\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhkl\u001b[0m (use `wandb login --relogin` to force relogin)\n"],"name":"stderr"},{"output_type":"stream","text":["env: WANDB_PROJECT=P2\n","env: WANDB_LOG_MODEL=true\n","env: WANDB_SILENT=true\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5e40f34f-64c1-49ff-acb6-0c3bafd961cb"},"source":["# Set Experiment\n","CFG.name = 'Baseline'\n","CFG.tag = ['Baseline']\n","CFG.NUM_FOLD = 5\n","CFG.FOLD = range(CFG.NUM_FOLD)   # if you want to do just simple test, set this [0]\n","\n","CFG.MODEL_NAME = \"xlm-roberta-large\"\n","CFG.lr = 3e-5\n","CFG.batch_size = 50\n","CFG.epochs = 10\n","CFG.classifier_dropout = 0.1\n","CFG.tokenizer_max_length = 150\n","CFG.weight_decay = 0.00\n","CFG.seed = 42\n","CFG.random_masking_rate = 0.1"],"id":"5e40f34f-64c1-49ff-acb6-0c3bafd961cb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"68526f18-6e91-4144-8fc6-5d2274a29da0"},"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(CFG.seed)"],"id":"68526f18-6e91-4144-8fc6-5d2274a29da0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5852c05-07d0-496a-9236-a45e26fc4756"},"source":["# Set Directory\n","if not os.path.isdir(f'custom_data/{CFG.name}') :\n","    os.chdir(os.path.join(ROOT_PATH, 'custom_data'))\n","    os.mkdir(f'{CFG.name}')\n","    os.chdir(ROOT_PATH)"],"id":"b5852c05-07d0-496a-9236-a45e26fc4756","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4e1f0de2-adf5-49bb-9b59-b04b65d423b6"},"source":["### Set Dataset"],"id":"4e1f0de2-adf5-49bb-9b59-b04b65d423b6"},{"cell_type":"code","metadata":{"id":"315c1337-df35-442b-8a9f-5b8e1972433a"},"source":["origin_dataset = pd.read_csv(os.path.join(ROOT_PATH, 'drive/MyDrive/P2/train.tsv'), delimiter='\\t', header=None)\n","en_dataset = pd.read_csv(os.path.join(ROOT_PATH, 'drive/MyDrive/P2/new_data_NaN_en.tsv'), delimiter='\\t', header=None)\n","ja_dataset = pd.read_csv(os.path.join(ROOT_PATH, 'drive/MyDrive/P2/new_data_NaN_ja.tsv'), delimiter='\\t', header=None) \n","zh_dataset = pd.read_csv(os.path.join(ROOT_PATH, 'drive/MyDrive/P2/new_data_NaN_zh.tsv'), delimiter='\\t', header=None)"],"id":"315c1337-df35-442b-8a9f-5b8e1972433a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a58ba142-1fef-4d27-aebe-7a3b7992b79d"},"source":["with open(os.path.join(ROOT_PATH, 'drive/MyDrive/P2/label_type.pkl'), 'rb') as f:\n","    label_type = pickle.load(f)\n","\n","error_label_0 = ['wikitree-12599-4-108-111-4-7',\n","                 'wikipedia-25967-115-24-26-35-37',\n","                 'wikipedia-16427-6-14-17-20-22',\n","                 'wikipedia-16427-8-0-3-26-28',\n","                 'wikitree-19765-5-30-33-6-8',\n","                 'wikitree-58702-0-18-20-22-24',\n","                 'wikitree-71638-8-21-23-15-17',\n","                 'wikipedia-257-0-0-1-53-57',\n","                 'wikipedia-13649-28-66-70-14-24',\n","                 'wikipedia-6017-8-20-26-4-7']\n","error_label_1 = ['wikitree-55837-4-0-2-10-11']\n","error_label_2 = ['wikitree-62775-3-3-7-0-2']\n","error_label_3 = ['wikipedia-23188-0-74-86-41-42']\n","\n","labels = []\n","for ID, i in zip(origin_dataset[0], origin_dataset[8]):\n","    if i == 'blind':\n","        labels.append(100)\n","    elif ID in error_label_0:\n","        labels.append(label_type['관계_없음'])\n","    elif ID in error_label_1:\n","        labels.append(label_type['단체:구성원'])\n","    elif ID in error_label_2:\n","        labels.append(label_type['단체:본사_도시'])\n","    elif ID in error_label_3:\n","        labels.append(label_type['단체:하위_단체'])\n","    else:\n","        labels.append(label_type[i])"],"id":"a58ba142-1fef-4d27-aebe-7a3b7992b79d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"39ace32b-d506-4379-8052-02887d1850df"},"source":["dataset = pd.DataFrame({\n","    'origin_sentence' : origin_dataset[1],\n","    'en_sentence' : en_dataset[1],\n","    'ja_sentence' : ja_dataset[1],\n","    'zh_sentence' : zh_dataset[1],\n","    'entity_01' : origin_dataset[2],\n","    'entity_02' : origin_dataset[5],\n","    'labels' : labels\n","})"],"id":"39ace32b-d506-4379-8052-02887d1850df","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"08838924-ef7c-4fd0-aa05-0915dee5f591"},"source":["class NLPDataset(Dataset) : \n","    def __init__(self, dataset, tokenizer, training=False, threshold=0.1) :\n","        self.origin_sentence = dataset['origin_sentence']\n","        self.en_sentence = dataset['en_sentence']\n","        self.ja_sentence = dataset['ja_sentence']\n","        self.zh_sentence = dataset['zh_sentence']\n","        self.entity_01 = dataset['entity_01']\n","        self.entity_02 = dataset['entity_02']\n","        self.labels = torch.tensor(dataset['labels'])\n","        self.tokenizer = tokenizer\n","        self.training = training\n","        self.threshold = threshold\n","        \n","    def __getitem__(self, idx) :\n","        if self.training :\n","            sentences = [self.origin_sentence[idx], ]\n","            if self._is_sentence(self.en_sentence[idx]) :\n","                sentences.append(self.en_sentence[idx])\n","            if self._is_sentence(self.ja_sentence[idx]) :\n","                sentences.append(self.ja_sentence[idx])\n","            if self._is_sentence(self.zh_sentence[idx]) :\n","                sentences.append(self.zh_sentence[idx])\n","            sentence = sentences[np.random.randint(len(sentences))]\n","        else :\n","            sentence = self.origin_sentence[idx]\n","        \n","        e1 = self.entity_01[idx]\n","        e2 = self.entity_02[idx]\n","        e1_mask, e2_mask = self._get_ent_mask(e1, e2)\n","\n","        item = tokenizer(e1+' RELATION '+e2, sentence, max_length=CFG.tokenizer_max_length, \n","                         padding='max_length', truncation=True, return_tensors='pt')\n","        item['input_ids'] = self._random_mask(item['input_ids'][0], e1, e2)\n","        item['attention_mask'] = item['attention_mask'].squeeze(0)\n","        item['e1_mask'] = torch.Tensor(e1_mask)\n","        item['e2_mask'] = torch.Tensor(e2_mask)\n","        item['labels'] = self.labels[idx]\n","        return item\n","        \n","    def __len__(self) :\n","        return len(self.labels)\n","    \n","    def _random_mask(self, sentence, e1, e2) :\n","        mask_id = self.tokenizer.encode('<mask>', add_special_tokens=False)\n","        important_tokens = self.tokenizer.all_special_ids\n","        important_tokens += tokenizer.encode('RELATION', add_special_tokens=False)\n","        important_tokens += tokenizer.encode(e1, e2, add_special_tokens=False)\n","        for i, token in enumerate(sentence) :\n","            if int(token) not in list(important_tokens) and self.threshold > random.random() :\n","                sentence[i] = mask_id[0]\n","        return sentence\n","    \n","    def _is_sentence(self, sentence) :\n","        return False if sentence is np.NaN else True\n","    \n","    def _get_ent_mask(self, e1, e2) :\n","        e1_mask = np.zeros(CFG.tokenizer_max_length, dtype=int)\n","        e2_mask = np.zeros(CFG.tokenizer_max_length, dtype=int)\n","        e1_len = len(self.tokenizer.encode(e1, add_special_tokens=False))\n","        e2_len = len(self.tokenizer.encode(e2, add_special_tokens=False))\n","        e1_mask[1 : 1+e1_len] = 1\n","        e2_mask[3+e1_len : 3+e1_len+e2_len] = 1\n","        return e1_mask, e2_mask"],"id":"08838924-ef7c-4fd0-aa05-0915dee5f591","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"82d2508d-6485-4d4e-859d-6718b9ba274a"},"source":["tokenizer = transformers.AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n","train_dataset = NLPDataset(dataset, tokenizer, training=True, threshold=CFG.random_masking_rate)\n","valid_dataset = NLPDataset(dataset, tokenizer, training=False, threshold=0.0)"],"id":"82d2508d-6485-4d4e-859d-6718b9ba274a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DQyp7oh-V_Xn"},"source":["### Set Validation"],"id":"DQyp7oh-V_Xn"},{"cell_type":"code","metadata":{"id":"6234f079-752e-43ac-a3f3-b6b0d31a352d"},"source":["skf = StratifiedKFold(n_splits=CFG.NUM_FOLD, shuffle=True, random_state=CFG.seed)\n","folds = []\n","for train_idx, valid_idx in skf.split(origin_dataset, labels) :\n","    folds.append({'train_idx':train_idx, 'valid_idx':valid_idx})"],"id":"6234f079-752e-43ac-a3f3-b6b0d31a352d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"73457f5a-a406-4883-adda-1ebab75cd5a6"},"source":["### Define Model"],"id":"73457f5a-a406-4883-adda-1ebab75cd5a6"},{"cell_type":"code","metadata":{"id":"7b488213-96dc-4f34-81c5-f595f0ed1d77"},"source":["### model architecture\n","class FCLayer(nn.Module):\n","    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n","        super(FCLayer, self).__init__()\n","        self.use_activation = use_activation\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.linear = nn.Linear(input_dim, output_dim)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        if self.use_activation:\n","            x = self.tanh(x)\n","        return self.linear(x)\n","\n","class RBERT_RobertaForSequenceClassification(nn.Module):\n","    def __init__(self, model_name, num_classes, dr_rate=0.1):\n","        super(RBERT_RobertaForSequenceClassification, self).__init__()\n","        \n","        config = transformers.AutoConfig.from_pretrained(model_name)\n","        config.num_labels = config.hidden_size\n","        self.backbone = transformers.AutoModel.from_pretrained(model_name, config=config)\n","        self.num_classes = num_classes\n","        self.dropout_rate = dr_rate\n","        \n","        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, self.dropout_rate)\n","        self.entity_fc_layer = FCLayer(config.hidden_size, config.hidden_size, self.dropout_rate)\n","        self.label_classifier = FCLayer(config.hidden_size*3, self.num_classes, self.dropout_rate, use_activation=False)\n","        \n","    def forward(self, input_ids, attention_mask, e1_mask, e2_mask, labels=None):\n","        outputs = self.backbone(input_ids=input_ids,\n","                                attention_mask=attention_mask)\n","        \n","        sequence_output = outputs['last_hidden_state']\n","        pooled_output = outputs['pooler_output']  # [CLS]\n","        \n","        e1_h = self.entity_average(sequence_output, e1_mask)\n","        e2_h = self.entity_average(sequence_output, e2_mask)\n","        \n","        pooled_output = self.cls_fc_layer(pooled_output)\n","        e1_h = self.entity_fc_layer(e1_h)\n","        e2_h = self.entity_fc_layer(e2_h)\n","        \n","        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)\n","        logits = self.label_classifier(concat_h)\n","        return logits\n","    \n","    def entity_average(self, hidden_output, e_mask):\n","        \"\"\"\n","        Average the entity hidden state vectors (H_i ~ H_j)\n","        :param hidden_output: [batch_size, j-i+1, dim]\n","        :param e_mask: [batch_size, max_seq_len]\n","                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]\n","        :return: [batch_size, dim]\n","        \"\"\"\n","        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n","        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n","\n","        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n","        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n","        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n","        return avg_vector"],"id":"7b488213-96dc-4f34-81c5-f595f0ed1d77","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5k1YXlJ3WE1K"},"source":["class LSTMModel(nn.Module):\n","    def __init__(self, model_name):\n","        super().__init__() \n","        self.transformer_model = AutoModel.from_pretrained(model_name, hidden_dropout_prob=0.2)\n","        self.lstm = nn.LSTM(input_size = 1024, hidden_size = 1024, num_layers = 3, dropout=0.5, bidirectional = True, batch_first = True)\n","        self.dense_layer = nn.Linear(2048, 42, bias=True)\n","        \n","    \n","    def forward(self, input_ids, attention_mask):\n","        encode_layers = self.transformer_model(input_ids=input_ids, attention_mask = attention_mask)[0]\n","        enc_hiddens, (last_hidden, last_cell) = self.lstm(encode_layers)\n","        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim = 1)\n","\n","        output = self.dense_layer(output_hidden)\n","\n","        return output"],"id":"5k1YXlJ3WE1K","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"91675d8e-878a-471e-89c9-a98b450b1d56"},"source":["### Define utils"],"id":"91675d8e-878a-471e-89c9-a98b450b1d56"},{"cell_type":"code","metadata":{"id":"86ddaaff-030d-406e-9f28-c2bbedd7af4c"},"source":["class FocalLoss(nn.Module):\n","    def __init__(self, gamma=0, alpha=None, size_average=True):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n","        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n","        self.size_average = size_average\n","\n","    def forward(self, input, target):\n","        if input.dim()>2:\n","            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n","            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n","            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n","        target = target.view(-1,1)\n","\n","        logpt = F.log_softmax(input)\n","        logpt = logpt.gather(1,target)\n","        logpt = logpt.view(-1)\n","        pt = Variable(logpt.data.exp())\n","\n","        if self.alpha is not None:\n","            if self.alpha.type()!=input.data.type():\n","                self.alpha = self.alpha.type_as(input.data)\n","            at = self.alpha.gather(0,target.data.view(-1))\n","            logpt = logpt * Variable(at)\n","\n","        loss = -1 * (1-pt)**self.gamma * logpt\n","        if self.size_average: return loss.mean()\n","        else: return loss.sum()"],"id":"86ddaaff-030d-406e-9f28-c2bbedd7af4c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fe1849ef-87bd-4429-be92-b07957ce2525"},"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"id":"fe1849ef-87bd-4429-be92-b07957ce2525","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ece39c63-3f5f-4234-bc00-679d0b6c16bc"},"source":["### Training"],"id":"ece39c63-3f5f-4234-bc00-679d0b6c16bc"},{"cell_type":"code","metadata":{"scrolled":true,"tags":[],"id":"a5f00d46-bb07-417b-a8dc-31911d47ea7a","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1619030283409,"user_tz":-540,"elapsed":4701399,"user":{"displayName":"이현규","photoUrl":"","userId":"11926500847670507435"}},"outputId":"599cb920-f219-4aee-8fd6-87676683f18d"},"source":["for fold in CFG.FOLD :\n","    train_idx, valid_idx = folds[fold]['train_idx'], folds[fold]['valid_idx']\n","    train_subset = Subset(train_dataset, train_idx)\n","    valid_subset = Subset(valid_dataset, valid_idx)\n","    train_loader = DataLoader(train_subset, batch_size=CFG.batch_size, shuffle=True, num_workers=5)\n","    valid_loader = DataLoader(valid_subset, batch_size=CFG.batch_size, shuffle=True, num_workers=5)\n","    \n","    model = RBERT_RobertaForSequenceClassification(CFG.MODEL_NAME, num_classes=42, dr_rate=CFG.classifier_dropout)\n","    model.to(device)\n","    optimizer = AdamP(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2250, eta_min=1e-6)\n","    loss_fn = FocalLoss(gamma=0.5)\n","    \n","    train_loss, train_acc = AverageMeter(), AverageMeter()\n","    valid_loss, valid_acc = AverageMeter(), AverageMeter()\n","    best_val_acc = 0\n","    steps = 0\n","    \n","    run = wandb.init(project='P2', group=CFG.MODEL_NAME, name=CFG.name, tags=CFG.tag, config=CFG)\n","    t = time.time()\n","    for epoch in range(CFG.epochs) :\n","        for item in train_loader :\n","            input_ids = item['input_ids'].to(device)\n","            attention_mask = item['attention_mask'].to(device)\n","            e1_mask = item['e1_mask'].to(device)\n","            e2_mask = item['e2_mask'].to(device)\n","            label = item['labels'].to(device)\n","\n","            model.train()\n","            logit = model(input_ids, attention_mask, e1_mask, e2_mask)\n","            loss = loss_fn(logit, label)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            pred = logit.argmax(-1)\n","            acc = (pred == label).sum().float() / input_ids.size(0)\n","            train_loss.update(loss.item(), input_ids.size(0))\n","            train_acc.update(acc, input_ids.size(0))\n","            wandb.log({'Step' : steps})\n","\n","            steps += 1\n","            if steps % 100 == 0 :\n","                for item in valid_loader :\n","                    input_ids = item['input_ids'].to(device)\n","                    attention_mask = item['attention_mask'].to(device)\n","                    e1_mask = item['e1_mask'].to(device)\n","                    e2_mask = item['e2_mask'].to(device)\n","                    label = item['labels'].to(device)\n","                    \n","                    model.eval()\n","                    logit = model(input_ids, attention_mask, e1_mask, e2_mask)\n","                    loss = loss_fn(logit, label)\n","                    \n","                    pred = logit.argmax(-1)\n","                    acc = (pred == label).sum().float() / input_ids.size(0)\n","                    valid_loss.update(loss.item(), input_ids.size(0))\n","                    valid_acc.update(acc, input_ids.size(0))\n","                \n","                print(f'steps:{steps}\\t| valid_acc:{valid_acc.avg:.4}\\t| valid_loss:{valid_loss.avg:.4}\\t| train_acc:{train_acc.avg:.4}\\t| train_loss:{train_loss.avg:.4}\\t| time:{time.time()-t}')\n","                wandb.log({\n","                    \"eval/accuracy\": valid_acc.avg,\n","                    \"eval/loss\": valid_loss.avg,\n","                    \"train/loss\": train_loss.avg,\n","                    \"train/learning_rate\": scheduler.get_last_lr()[0]\n","                })\n","                      \n","                if valid_acc.avg > best_val_acc :\n","                    best_val_acc = valid_acc.avg\n","                    for f in glob.glob(f'custom_data/{CFG.name}/{fold}_*{CFG.name}.pth') :\n","                        open(f, 'w').close()\n","                        os.remove(f)\n","                    torch.save(model.state_dict(), os.path.join(ROOT_PATH, 'custom_data', CFG.name, f'{fold}_{steps}_{best_val_acc:.4}_{CFG.name}.pth'))\n","                      \n","                valid_acc.reset()\n","                valid_loss.reset()\n","                train_acc.reset()\n","                train_loss.reset()\n","                t = time.time()"],"id":"a5f00d46-bb07-417b-a8dc-31911d47ea7a","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","                Tracking run with wandb version 0.10.27<br/>\n","                Syncing run <strong style=\"color:#cdcd00\">RBERT-batch10-tanh</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://wandb.ai/hkl/P2\" target=\"_blank\">https://wandb.ai/hkl/P2</a><br/>\n","                Run page: <a href=\"https://wandb.ai/hkl/P2/runs/215c1ab2\" target=\"_blank\">https://wandb.ai/hkl/P2/runs/215c1ab2</a><br/>\n","                Run data is saved locally in <code>/content/wandb/run-20210421_172019-215c1ab2</code><br/><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["steps:100\t| valid_acc:0.5694\t| valid_loss:1.631\t| train_acc:0.51\t| train_loss:2.043\t| time:104.67896699905396\n","steps:200\t| valid_acc:0.5989\t| valid_loss:1.366\t| train_acc:0.526\t| train_loss:1.76\t| time:104.23421621322632\n","steps:300\t| valid_acc:0.6156\t| valid_loss:1.251\t| train_acc:0.55\t| train_loss:1.537\t| time:104.51554346084595\n","steps:400\t| valid_acc:0.6161\t| valid_loss:1.139\t| train_acc:0.578\t| train_loss:1.355\t| time:104.36885523796082\n","steps:500\t| valid_acc:0.6206\t| valid_loss:1.132\t| train_acc:0.612\t| train_loss:1.242\t| time:104.8938398361206\n","steps:600\t| valid_acc:0.6467\t| valid_loss:1.019\t| train_acc:0.637\t| train_loss:1.064\t| time:104.8907961845398\n","steps:700\t| valid_acc:0.6472\t| valid_loss:0.9621\t| train_acc:0.608\t| train_loss:1.222\t| time:104.79438209533691\n","steps:800\t| valid_acc:0.6328\t| valid_loss:1.005\t| train_acc:0.628\t| train_loss:1.051\t| time:105.23859357833862\n","steps:900\t| valid_acc:0.6711\t| valid_loss:0.8921\t| train_acc:0.626\t| train_loss:1.027\t| time:104.58516383171082\n","steps:1000\t| valid_acc:0.6594\t| valid_loss:0.9393\t| train_acc:0.672\t| train_loss:0.9307\t| time:104.61024522781372\n","steps:1100\t| valid_acc:0.6867\t| valid_loss:0.8258\t| train_acc:0.662\t| train_loss:0.8938\t| time:104.47193956375122\n","steps:1200\t| valid_acc:0.6872\t| valid_loss:0.8244\t| train_acc:0.691\t| train_loss:0.8496\t| time:104.66030550003052\n","steps:1300\t| valid_acc:0.7\t| valid_loss:0.7905\t| train_acc:0.691\t| train_loss:0.8342\t| time:104.51623034477234\n","steps:1400\t| valid_acc:0.7089\t| valid_loss:0.7633\t| train_acc:0.694\t| train_loss:0.8093\t| time:104.53870987892151\n","steps:1500\t| valid_acc:0.7139\t| valid_loss:0.7728\t| train_acc:0.71\t| train_loss:0.7979\t| time:104.97279667854309\n","steps:1600\t| valid_acc:0.7128\t| valid_loss:0.7594\t| train_acc:0.738\t| train_loss:0.6821\t| time:104.34448432922363\n","steps:1700\t| valid_acc:0.7239\t| valid_loss:0.7173\t| train_acc:0.729\t| train_loss:0.6993\t| time:104.36515045166016\n","steps:1800\t| valid_acc:0.7261\t| valid_loss:0.7137\t| train_acc:0.753\t| train_loss:0.6213\t| time:104.6224274635315\n","steps:1900\t| valid_acc:0.725\t| valid_loss:0.7113\t| train_acc:0.766\t| train_loss:0.6319\t| time:104.53355884552002\n","steps:2000\t| valid_acc:0.7311\t| valid_loss:0.6976\t| train_acc:0.72\t| train_loss:0.7273\t| time:104.30835342407227\n","steps:2100\t| valid_acc:0.7367\t| valid_loss:0.6835\t| train_acc:0.736\t| train_loss:0.6589\t| time:104.39915728569031\n","steps:2200\t| valid_acc:0.7406\t| valid_loss:0.6865\t| train_acc:0.731\t| train_loss:0.6587\t| time:104.9209291934967\n","steps:2300\t| valid_acc:0.7394\t| valid_loss:0.6941\t| train_acc:0.79\t| train_loss:0.5687\t| time:104.38055396080017\n","steps:2400\t| valid_acc:0.7478\t| valid_loss:0.6893\t| train_acc:0.767\t| train_loss:0.5683\t| time:104.4079110622406\n","steps:2500\t| valid_acc:0.7411\t| valid_loss:0.7008\t| train_acc:0.768\t| train_loss:0.5841\t| time:104.44104838371277\n","steps:2600\t| valid_acc:0.7406\t| valid_loss:0.6858\t| train_acc:0.771\t| train_loss:0.5933\t| time:104.63747525215149\n","steps:2700\t| valid_acc:0.7372\t| valid_loss:0.6943\t| train_acc:0.749\t| train_loss:0.64\t| time:104.54049873352051\n","steps:2800\t| valid_acc:0.7289\t| valid_loss:0.7165\t| train_acc:0.753\t| train_loss:0.6226\t| time:104.50454258918762\n","steps:2900\t| valid_acc:0.7261\t| valid_loss:0.7217\t| train_acc:0.803\t| train_loss:0.5233\t| time:104.91011595726013\n","steps:3000\t| valid_acc:0.7361\t| valid_loss:0.6872\t| train_acc:0.774\t| train_loss:0.5822\t| time:104.18194913864136\n","steps:3100\t| valid_acc:0.7306\t| valid_loss:0.7193\t| train_acc:0.735\t| train_loss:0.6609\t| time:104.34519386291504\n","steps:3200\t| valid_acc:0.7217\t| valid_loss:0.7242\t| train_acc:0.798\t| train_loss:0.5479\t| time:104.55051136016846\n","steps:3300\t| valid_acc:0.6878\t| valid_loss:0.8277\t| train_acc:0.723\t| train_loss:0.6858\t| time:104.30739378929138\n","steps:3400\t| valid_acc:0.71\t| valid_loss:0.7867\t| train_acc:0.737\t| train_loss:0.6662\t| time:104.31569576263428\n","steps:3500\t| valid_acc:0.7089\t| valid_loss:0.7468\t| train_acc:0.741\t| train_loss:0.6226\t| time:104.30136847496033\n","steps:3600\t| valid_acc:0.7239\t| valid_loss:0.7003\t| train_acc:0.723\t| train_loss:0.6776\t| time:104.23773694038391\n","steps:3700\t| valid_acc:0.6978\t| valid_loss:0.79\t| train_acc:0.754\t| train_loss:0.611\t| time:104.84190464019775\n","steps:3800\t| valid_acc:0.7078\t| valid_loss:0.8077\t| train_acc:0.74\t| train_loss:0.6975\t| time:104.24513483047485\n","steps:3900\t| valid_acc:0.7233\t| valid_loss:0.7696\t| train_acc:0.749\t| train_loss:0.6157\t| time:104.2343978881836\n","steps:4000\t| valid_acc:0.7222\t| valid_loss:0.7846\t| train_acc:0.755\t| train_loss:0.6562\t| time:104.14369034767151\n","steps:4100\t| valid_acc:0.72\t| valid_loss:0.7212\t| train_acc:0.741\t| train_loss:0.6382\t| time:104.37524247169495\n","steps:4200\t| valid_acc:0.69\t| valid_loss:0.7752\t| train_acc:0.714\t| train_loss:0.7361\t| time:104.35220289230347\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-c9f07217bd14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                     \u001b[0mvalid_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"b7a6fd70-fd57-4da3-9ba9-34f23aecd8ec"},"source":["### Inference and Ensemble"],"id":"b7a6fd70-fd57-4da3-9ba9-34f23aecd8ec"},{"cell_type":"code","metadata":{"id":"8e7dbfcd-4c2a-4bfe-8b0d-132ddf0751f9"},"source":["class TestDataset(Dataset) :\n","    def __init__(self, file_dir, tokenizer) :\n","        self.dataset = pd.read_csv(file_dir, delimiter='\\t', header=None)\n","        self.sentence = self.dataset[1]\n","        self.entity_01 = self.dataset[2]\n","        self.entity_02 = self.dataset[5]\n","        self.tokenizer = tokenizer\n","    \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","    def __getitem__(self, idx) :\n","        sentence = self.sentence[idx]\n","        e1 = self.entity_01[idx]\n","        e2 = self.entity_02[idx]\n","        e1_mask, e2_mask = self._get_ent_mask(e1, e2)\n","        item = tokenizer(e1+' RELATION '+e2, sentence, max_length=CFG.tokenizer_max_length, \n","                         padding='max_length', truncation=True, return_tensors='pt')\n","        item['input_ids'] = item['input_ids'].squeeze(0)\n","        item['attention_mask'] = item['attention_mask'].squeeze(0)\n","        item['e1_mask'] = torch.Tensor(e1_mask)\n","        item['e2_mask'] = torch.Tensor(e2_mask)\n","        return item\n","    \n","    def _get_ent_mask(self, e1, e2) :\n","        e1_mask = np.zeros(CFG.tokenizer_max_length, dtype=int)\n","        e2_mask = np.zeros(CFG.tokenizer_max_length, dtype=int)\n","        e1_len = len(self.tokenizer.encode(e1, add_special_tokens=False))\n","        e2_len = len(self.tokenizer.encode(e2, add_special_tokens=False))\n","        e1_mask[1 : 1+e1_len] = 1\n","        e2_mask[3+e1_len : 3+e1_len+e2_len] = 1\n","        return e1_mask, e2_mask"],"id":"8e7dbfcd-4c2a-4bfe-8b0d-132ddf0751f9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9e73b087-5948-4f1b-9cad-6e7ddcae24e6"},"source":["test_dataset = TestDataset(\"/opt/ml/input/data/test/test.tsv\", tokenizer)\n","test_loader = DataLoader(test_dataset, 10, shuffle=False)\n","\n","probs_lst = []\n","for best_model in glob.glob(f'custom_data/{CFG.name}/*{CFG.name}.pth') :\n","    model = RBERT_RobertaForSequenceClassification(CFG.MODEL_NAME, 42)\n","    model.load_state_dict(torch.load(best_model))\n","    model.to(device)\n","    \n","    output_probs = []\n","    for item in test_loader :\n","        input_ids = item['input_ids'].to(device)\n","        attention_mask = item['attention_mask'].to(device)\n","        e1_mask = item['e1_mask'].to(device)\n","        e2_mask = item['e2_mask'].to(device)\n","        \n","        model.eval()\n","        logit = model(input_ids, attention_mask, e1_mask, e2_mask)\n","        output_probs.extend(logit.cpu().detach().numpy())\n","    \n","    output_probs = torch.nn.functional.softmax(torch.Tensor(output_probs), dim=1)\n","    probs_lst.append(np.array(output_probs)[...,np.newaxis])\n","\n","models_prob = np.mean(np.concatenate(probs_lst, axis=2), axis=2)\n","np.save(os.path.join(ROOT_PATH, 'custom_data', CFG.name, f'Probs_{CFG.name}.npy'), models_prob)\n","\n","models_pred = np.argmax(models_prob, axis=1)\n","output = pd.DataFrame(models_pred, columns=['pred'])\n","output.to_csv(os.path.join(ROOT_PATH, 'custom_data', CFG.name, f'Basic_{CFG.name}.csv'), index=False)\n","\n","models_prob += np.array([0.1] + [0]*41)\n","models_pred = np.argmax(models_prob, axis=1)\n","output = pd.DataFrame(models_pred, columns=['pred'])\n","output.to_csv(os.path.join(ROOT_PATH, 'custom_data', CFG.name, f'AddWeight_{CFG.name}.csv'), index=False)"],"id":"9e73b087-5948-4f1b-9cad-6e7ddcae24e6","execution_count":null,"outputs":[]}]}